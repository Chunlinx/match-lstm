{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.vectorize import vectorize\n",
    "ddict = vectorize()\n",
    "vocab_size = len(list(ddict['word2index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 4\n",
    "sentence_size = 30\n",
    "embedding_size = 300\n",
    "batch_size = 8\n",
    "num_class = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "premises = tf.placeholder(shape=[None, sentence_size], dtype=tf.int32, name='P')\n",
    "hypotheses = tf.placeholder(shape=[None, sentence_size], dtype=tf.int32, name='H')\n",
    "labels = tf.placeholder(shape=[None, num_class ] , dtype=tf.int32, name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embed line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_line(i, inputs, embeddings, emb_ta):\n",
    "    emb_list = []\n",
    "    for j in range(sentence_size):\n",
    "        word = inputs[i][j]\n",
    "        unk_word = tf.constant(-1)\n",
    "        f1 = lambda : tf.nn.embedding_lookup(params=emb, ids=word)\n",
    "        f2 = lambda : tf.zeros(shape=[embedding_size])\n",
    "        word_emb = tf.case([(tf.not_equal(unk_word, word), f1)], default=f2)\n",
    "        emb_list.append(word_emb)\n",
    "    emb_tensor = tf.stack(emb_list)\n",
    "    emb_ta = emb_ta.write(i, emb_tensor)\n",
    "    i = tf.add(i,1)\n",
    "    return i, inputs, embeddings, emb_ta\n",
    "\n",
    "\n",
    "# In[7]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_sentences(sentences, embeddings):\n",
    "    emb_ta = tf.TensorArray(dtype=tf.float32, size=batch_size)\n",
    "    i = tf.constant(0)\n",
    "    c = lambda x,y,z,n : tf.less(x, batch_size)\n",
    "    b = lambda x,y,z,n : embed_line(x,y,z,n)\n",
    "    emb_res = tf.while_loop(cond=c, body=b, loop_vars=(i, sentences, emb, emb_ta) )\n",
    "    emb_tensor = emb_res[-1].stack()\n",
    "    return tf.reshape(emb_tensor, [-1, sentence_size, embedding_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = tf.get_variable(name='emb', shape=[vocab_size, embedding_size])\n",
    "\n",
    "p_embs = embed_sentences(premises, emb)\n",
    "h_embs = embed_sentences(hypotheses, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequence length helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq_len(seq):\n",
    "    seq_bool = tf.sign(tf.abs(seq))\n",
    "    return tf.reduce_sum(seq_bool, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('lstm_premises'):\n",
    "    lstm_p_cell = tf.contrib.rnn.BasicLSTMCell(num_units=embedding_size)\n",
    "    # get actual length of premises\n",
    "    premises_len = seq_len(premises)\n",
    "    h_s, _ = tf.nn.dynamic_rnn(cell=lstm_p_cell, inputs=p_embs, sequence_length=premises_len,\n",
    "                     dtype=tf.float32)\n",
    "\n",
    "with tf.variable_scope('lstm_hypotheses'):\n",
    "    lstm_h_cell = tf.contrib.rnn.BasicLSTMCell(num_units=embedding_size)\n",
    "    # get actual length of premises\n",
    "    hyp_len = seq_len(hypotheses)\n",
    "    h_t, _ = tf.nn.dynamic_rnn(cell=lstm_h_cell, inputs=h_embs, sequence_length=hyp_len,\n",
    "                     dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lstm_m_cell = tf.contrib.rnn.BasicLSTMCell(num_units=embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def match_attention(k, p_emb, h_emb, len_p, state):\n",
    "    h_emb_k = tf.reshape(h_emb[k], [1, -1])\n",
    "    p_emb_k = tf.slice(p_emb, begin=[0,0], size=[len_p, embedding_size])\n",
    "    \n",
    "    with tf.variable_scope('attn_weights'):\n",
    "        w_s = tf.get_variable(shape=[embedding_size, embedding_size],\n",
    "                               name='w_s')\n",
    "        w_t = tf.get_variable(shape=[embedding_size, embedding_size],\n",
    "                               name='w_t')\n",
    "        w_m = tf.get_variable(shape=[embedding_size, embedding_size],\n",
    "                               name='w_m')\n",
    "        w_e = tf.get_variable(shape=[embedding_size, 1],\n",
    "                              name='w_e')\n",
    "    m_lstm_state = state.h\n",
    "    sum_m = tf.matmul(p_emb_k, w_s) + tf.matmul(h_emb_k, w_t) + tf.matmul(m_lstm_state, w_m)\n",
    "    alpha_k = tf.nn.softmax(tf.matmul(tf.tanh(sum_m), w_e))\n",
    "    a_k = tf.matmul(alpha_k, p_emb_k, transpose_a=True)\n",
    "    a_k.set_shape([1, embedding_size])\n",
    "    \n",
    "    m_k = tf.concat([a_k, h_emb_k], axis=1)\n",
    "    with tf.variable_scope('lstm_m_step'):\n",
    "        _, next_state = lstm_m_cell(inputs=m_k, state=state)\n",
    "    \n",
    "    k = tf.add(k,1)\n",
    "    \n",
    "    return k, p_emb, h_emb, len_p, next_state\n",
    "\n",
    "def match_sentence(i, h_m_ta):\n",
    "    p_emb_i, h_emb_i = p_embs[i], h_embs[i]\n",
    "    len_p_i, len_h_i = seq_len(premises[i]), seq_len(hypotheses[i])\n",
    "    state = lstm_m_cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "    \n",
    "    # inner loop\n",
    "    k = tf.constant(0)\n",
    "    c = lambda a, x, y, z, s : tf.less(a, len_h_i)\n",
    "    b = lambda a,x,y,z,s : match_attention(a,x,y,z,s)\n",
    "    res = tf.while_loop(cond=c, body=b, \n",
    "                       loop_vars=(k, p_emb_i, h_emb_i, len_p_i, state ))\n",
    "    \n",
    "    h_m_ta = h_m_ta.write(i, res[-1].h)\n",
    "    \n",
    "    i = tf.add(i,1)\n",
    "    return i, h_m_ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.variable_scope('lstm_m'):\n",
    "    h_m_ta = tf.TensorArray(dtype=tf.float32, size=batch_size)\n",
    "    c = lambda x, y : tf.less(x, batch_size)\n",
    "    b = lambda x, y : match_sentence(x,y)\n",
    "    i = tf.constant(0)\n",
    "    h_m_res = tf.while_loop(cond=c, body=b,\n",
    "                           loop_vars = (i, h_m_ta))\n",
    "    \n",
    "    h_m_tensor = tf.squeeze(h_m_res[-1].stack(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  with tf.variable_scope('fully_connected'):\n",
    "    w_fc = tf.get_variable(shape=[embedding_size,num_class], name='w_fc')\n",
    "    b_fc = tf.get_variable(shape=[num_class], name='b_fc')\n",
    "    logits = tf.matmul(h_m_tensor, w_fc) + b_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from lib.data_utils import unpack_data\n",
    "train_data = ddict['train_data']\n",
    "data = unpack_data(train_data[:8], num_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_v, logits_v = sess.run([loss, logits], feed_dict= {\n",
    "    premises : data[0],\n",
    "    hypotheses : data[1],\n",
    "    labels : data[3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.48950595, -0.75676656,  0.53591716],\n",
       "       [-0.48584241, -0.75595647,  0.53654009],\n",
       "       [-0.49568334, -0.75488168,  0.53990406],\n",
       "       [-0.50009775, -0.75139207,  0.55102873],\n",
       "       [-0.5093928 , -0.74577904,  0.55475366],\n",
       "       [-0.50274116, -0.74918902,  0.55480289],\n",
       "       [-0.52755791, -0.73060483,  0.54057616],\n",
       "       [-0.52656943, -0.72776276,  0.54551983]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
